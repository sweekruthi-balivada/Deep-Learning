{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BHtNDeBcIy7"
      },
      "outputs": [],
      "source": [
        "!pip install -U portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "q0IFIXQZcjkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations --> Took reference from the Demo\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize data to range [-1, 1]\n",
        "])\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Training and Testing loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WwHFxEcjnr",
        "outputId": "ecbe6025-e78a-409e-875a-d9716a3fe4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 147195148.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 25262918.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 40290188.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21333178.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping the labels for the MNIST dataset\n",
        "labels_map = {\n",
        "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
        "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
        "}"
      ],
      "metadata": {
        "id": "Q3vJqG-scjrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "\n",
        "# Displaying figures from the dataset randomly\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    img, label = train_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "SQU1o3POdqT6",
        "outputId": "896cf2ef-0d6c-4183-9299-6c92fdc6829f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model building"
      ],
      "metadata": {
        "id": "ab3YOOIyeDR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining NN architecure - MLP- Multi-layer Perceptron\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128) ## First fully connected linear layer, 28*28 I/P features and 128 O/P features\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10) ## 10 o/p features because MNIST has 10 O/P features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x)) ## Applying relu activation for the first layer\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initializing the neural network\n",
        "model = MLP()"
      ],
      "metadata": {
        "id": "g8fqNDq3eCj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u488QnzVe40V",
        "outputId": "07b39dfe-b325-4884-fd33-9a58a7514c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.027766463458538\n",
            "Epoch 1, Batch 200, Loss: 0.42261952549219134\n",
            "Epoch 1, Batch 300, Loss: 0.36632166340947153\n",
            "Epoch 1, Batch 400, Loss: 0.3557272307574749\n",
            "Epoch 1, Batch 500, Loss: 0.32772814996540545\n",
            "Epoch 1, Batch 600, Loss: 0.29438870944082735\n",
            "Epoch 1, Batch 700, Loss: 0.2688548056781292\n",
            "Epoch 1, Batch 800, Loss: 0.27697311133146285\n",
            "Epoch 1, Batch 900, Loss: 0.2461575023829937\n",
            "Epoch 2, Batch 100, Loss: 0.2318267051130533\n",
            "Epoch 2, Batch 200, Loss: 0.1971542874723673\n",
            "Epoch 2, Batch 300, Loss: 0.20060538444668055\n",
            "Epoch 2, Batch 400, Loss: 0.18731677621603013\n",
            "Epoch 2, Batch 500, Loss: 0.1862528335303068\n",
            "Epoch 2, Batch 600, Loss: 0.18688635274767876\n",
            "Epoch 2, Batch 700, Loss: 0.19139392241835596\n",
            "Epoch 2, Batch 800, Loss: 0.16494716588407754\n",
            "Epoch 2, Batch 900, Loss: 0.16701194643974304\n",
            "Epoch 3, Batch 100, Loss: 0.14217018101364373\n",
            "Epoch 3, Batch 200, Loss: 0.1357156409882009\n",
            "Epoch 3, Batch 300, Loss: 0.14191300103440882\n",
            "Epoch 3, Batch 400, Loss: 0.13317315870895982\n",
            "Epoch 3, Batch 500, Loss: 0.1306048571690917\n",
            "Epoch 3, Batch 600, Loss: 0.13653858641162514\n",
            "Epoch 3, Batch 700, Loss: 0.13610220907256007\n",
            "Epoch 3, Batch 800, Loss: 0.1328659082017839\n",
            "Epoch 3, Batch 900, Loss: 0.12809445071965456\n",
            "Epoch 4, Batch 100, Loss: 0.10921051470562816\n",
            "Epoch 4, Batch 200, Loss: 0.11545455377548933\n",
            "Epoch 4, Batch 300, Loss: 0.09679131466895342\n",
            "Epoch 4, Batch 400, Loss: 0.11692221987992525\n",
            "Epoch 4, Batch 500, Loss: 0.10911627040244638\n",
            "Epoch 4, Batch 600, Loss: 0.11043439267203212\n",
            "Epoch 4, Batch 700, Loss: 0.09866546832956374\n",
            "Epoch 4, Batch 800, Loss: 0.10773236274719239\n",
            "Epoch 4, Batch 900, Loss: 0.11210467977449298\n",
            "Epoch 5, Batch 100, Loss: 0.08945131720975041\n",
            "Epoch 5, Batch 200, Loss: 0.10023625827394426\n",
            "Epoch 5, Batch 300, Loss: 0.0968266783002764\n",
            "Epoch 5, Batch 400, Loss: 0.09499099906533956\n",
            "Epoch 5, Batch 500, Loss: 0.0803088225517422\n",
            "Epoch 5, Batch 600, Loss: 0.09136386956553906\n",
            "Epoch 5, Batch 700, Loss: 0.09221564980689437\n",
            "Epoch 5, Batch 800, Loss: 0.0888225799240172\n",
            "Epoch 5, Batch 900, Loss: 0.09303243995644152\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcYJzrWe46p",
        "outputId": "61227113-5f3c-4418-e5ef-2e7d7aa47e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.9676%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model by printing a random image and checking whether it correctly labelled or not\n",
        "image_index = 100\n",
        "test_image, test_label = test_dataset[image_index]\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(test_image.unsqueeze(0))\n",
        "    _, predicted_label = torch.max(output, 1)\n",
        "\n",
        "test_image_numpy = test_image.squeeze().numpy()\n",
        "\n",
        "plt.imshow(test_image_numpy, cmap='gray')\n",
        "plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "94n0j-djfYMN",
        "outputId": "da8a959d-bab9-450a-c922-5985a28e5ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4:\n",
        "\n",
        "### Hypothesis:\n",
        "Adding an additional dense layer will likely increase the model's ability to capture the latent and intricate details peresent in the data there by increasing the performance. But if there are more additional layers and the model is more complex there is a risk of overfitting, given the size of the dataset."
      ],
      "metadata": {
        "id": "nyPGLrzFgXP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the same neural network architecture but with one more layer\n",
        "class MLP_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_2, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)  # Adding another layer\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model 2\n",
        "model_2 = MLP_2()\n"
      ],
      "metadata": {
        "id": "TBC1x6VBgWfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function and optimizer are same\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)  # Use model_2 instead of model\n",
        "\n",
        "# Training the modified neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model_2.train()  # Ensure model_2 is in training mode\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_2(inputs)  # Use model_2 instead of model\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jblPoOG-gWjq",
        "outputId": "5cf157a1-5dae-42d2-b8b6-7c61bcd5cbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.076742314696312\n",
            "Epoch 1, Batch 200, Loss: 0.4475471602380276\n",
            "Epoch 1, Batch 300, Loss: 0.3799269212782383\n",
            "Epoch 1, Batch 400, Loss: 0.34226983912289144\n",
            "Epoch 1, Batch 500, Loss: 0.28865853302180766\n",
            "Epoch 1, Batch 600, Loss: 0.2841735565662384\n",
            "Epoch 1, Batch 700, Loss: 0.2699821048229933\n",
            "Epoch 1, Batch 800, Loss: 0.24283235229551792\n",
            "Epoch 1, Batch 900, Loss: 0.2421476786583662\n",
            "Epoch 2, Batch 100, Loss: 0.20647274751216174\n",
            "Epoch 2, Batch 200, Loss: 0.2019443818554282\n",
            "Epoch 2, Batch 300, Loss: 0.19839715786278248\n",
            "Epoch 2, Batch 400, Loss: 0.18704855311661958\n",
            "Epoch 2, Batch 500, Loss: 0.1912871154025197\n",
            "Epoch 2, Batch 600, Loss: 0.18548427280038596\n",
            "Epoch 2, Batch 700, Loss: 0.16605669908225537\n",
            "Epoch 2, Batch 800, Loss: 0.16753259748220445\n",
            "Epoch 2, Batch 900, Loss: 0.17385751377791167\n",
            "Epoch 3, Batch 100, Loss: 0.14374211695045233\n",
            "Epoch 3, Batch 200, Loss: 0.13203632552176714\n",
            "Epoch 3, Batch 300, Loss: 0.1296660495735705\n",
            "Epoch 3, Batch 400, Loss: 0.1367923179268837\n",
            "Epoch 3, Batch 500, Loss: 0.13350386634469033\n",
            "Epoch 3, Batch 600, Loss: 0.12340519607067107\n",
            "Epoch 3, Batch 700, Loss: 0.13936262214556336\n",
            "Epoch 3, Batch 800, Loss: 0.12942225815728306\n",
            "Epoch 3, Batch 900, Loss: 0.14398193471133708\n",
            "Epoch 4, Batch 100, Loss: 0.11467683685943485\n",
            "Epoch 4, Batch 200, Loss: 0.10318851245567202\n",
            "Epoch 4, Batch 300, Loss: 0.10919817247428\n",
            "Epoch 4, Batch 400, Loss: 0.10977426678873599\n",
            "Epoch 4, Batch 500, Loss: 0.12491055839695037\n",
            "Epoch 4, Batch 600, Loss: 0.12250848465599119\n",
            "Epoch 4, Batch 700, Loss: 0.09806926608085632\n",
            "Epoch 4, Batch 800, Loss: 0.11651577532291413\n",
            "Epoch 4, Batch 900, Loss: 0.10501116506755352\n",
            "Epoch 5, Batch 100, Loss: 0.09327204280532897\n",
            "Epoch 5, Batch 200, Loss: 0.08158763124141842\n",
            "Epoch 5, Batch 300, Loss: 0.09994460098445415\n",
            "Epoch 5, Batch 400, Loss: 0.10887346632778644\n",
            "Epoch 5, Batch 500, Loss: 0.08675765252904967\n",
            "Epoch 5, Batch 600, Loss: 0.0956259390199557\n",
            "Epoch 5, Batch 700, Loss: 0.09605947469361126\n",
            "Epoch 5, Batch 800, Loss: 0.09172520807944239\n",
            "Epoch 5, Batch 900, Loss: 0.09722657521255314\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "model_2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2DAi-dAgWnK",
        "outputId": "dc1019fc-e2dd-4fdb-944a-5264dd239ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.9662%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result:\n",
        "\n",
        "The result is in support to the hypothesis. The model where I have added an extra layer, did show a slight improvement in the accuracy when compared to the precious model."
      ],
      "metadata": {
        "id": "sIHWzUcMhpsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7\n",
        "Experimenting with the model"
      ],
      "metadata": {
        "id": "StM-W717lBTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Hypothesis 1:</b>\n",
        "\n",
        "Changing the optimizer to SGD, Increasing the learning rate to 0.01 may lead to faster learning during the training of the model.\n"
      ],
      "metadata": {
        "id": "7yKaQkQNpE4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the optimizer for Model_2\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_2.parameters(), lr=0.01) # Increased learning rate to 0.01%\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model_2.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "# Evaluating the model\n",
        "model_2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uDvINVYhrob",
        "outputId": "3f80a924-fc82-4482-b8fe-3b8ebde12c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.06339370383415371\n",
            "Epoch 1, Batch 200, Loss: 0.06208181254565716\n",
            "Epoch 1, Batch 300, Loss: 0.047062768893083556\n",
            "Epoch 1, Batch 400, Loss: 0.05031040196307004\n",
            "Epoch 1, Batch 500, Loss: 0.04745570327155292\n",
            "Epoch 1, Batch 600, Loss: 0.05178708768682554\n",
            "Epoch 1, Batch 700, Loss: 0.0489665582543239\n",
            "Epoch 1, Batch 800, Loss: 0.05437368585495278\n",
            "Epoch 1, Batch 900, Loss: 0.0529392583318986\n",
            "Epoch 2, Batch 100, Loss: 0.04382604194339365\n",
            "Epoch 2, Batch 200, Loss: 0.055102688455954195\n",
            "Epoch 2, Batch 300, Loss: 0.046025687910150735\n",
            "Epoch 2, Batch 400, Loss: 0.03759412131737918\n",
            "Epoch 2, Batch 500, Loss: 0.05094301956705749\n",
            "Epoch 2, Batch 600, Loss: 0.053495346810668705\n",
            "Epoch 2, Batch 700, Loss: 0.04265960224205628\n",
            "Epoch 2, Batch 800, Loss: 0.03871196806896478\n",
            "Epoch 2, Batch 900, Loss: 0.05338339628186077\n",
            "Epoch 3, Batch 100, Loss: 0.04099106887355447\n",
            "Epoch 3, Batch 200, Loss: 0.04021720333956182\n",
            "Epoch 3, Batch 300, Loss: 0.04734326071571559\n",
            "Epoch 3, Batch 400, Loss: 0.03677526771556586\n",
            "Epoch 3, Batch 500, Loss: 0.047348796548321845\n",
            "Epoch 3, Batch 600, Loss: 0.048179349924903365\n",
            "Epoch 3, Batch 700, Loss: 0.05102196611464024\n",
            "Epoch 3, Batch 800, Loss: 0.04163270883262157\n",
            "Epoch 3, Batch 900, Loss: 0.04340315364534035\n",
            "Epoch 4, Batch 100, Loss: 0.043758353509474546\n",
            "Epoch 4, Batch 200, Loss: 0.04170417972607538\n",
            "Epoch 4, Batch 300, Loss: 0.0395737659977749\n",
            "Epoch 4, Batch 400, Loss: 0.0424772267555818\n",
            "Epoch 4, Batch 500, Loss: 0.045385597348213194\n",
            "Epoch 4, Batch 600, Loss: 0.04618685118854046\n",
            "Epoch 4, Batch 700, Loss: 0.045156903171446175\n",
            "Epoch 4, Batch 800, Loss: 0.0461813004151918\n",
            "Epoch 4, Batch 900, Loss: 0.03219502500258386\n",
            "Epoch 5, Batch 100, Loss: 0.045194530980661514\n",
            "Epoch 5, Batch 200, Loss: 0.038900895045371725\n",
            "Epoch 5, Batch 300, Loss: 0.04339530626311898\n",
            "Epoch 5, Batch 400, Loss: 0.04007726325304248\n",
            "Epoch 5, Batch 500, Loss: 0.041819844667334106\n",
            "Epoch 5, Batch 600, Loss: 0.04344337101327255\n",
            "Epoch 5, Batch 700, Loss: 0.03711562843993306\n",
            "Epoch 5, Batch 800, Loss: 0.043429003442870456\n",
            "Epoch 5, Batch 900, Loss: 0.0364480765000917\n",
            "Finished Training\n",
            "Accuracy on test set: 0.9773%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result:\n",
        "\n",
        "Accuracy increased than the previous model. The learning rate as I have thought in the hypothesis, increased and lead to a faster training and lead to a better performance of the model."
      ],
      "metadata": {
        "id": "033ybOdtprNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Hypothesis 2:</b>\n",
        "\n",
        "Using a different activation function in the model. By using different activation functions, I want to see thier effect on the performance of the model.\n"
      ],
      "metadata": {
        "id": "1N5dwObxb473"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_3, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))  # Using sigmoid activation function for the second layer\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model_3\n",
        "model_3 = MLP_3()\n",
        "\n"
      ],
      "metadata": {
        "id": "FpuOOPLKhEGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_3.parameters(), lr=0.001)\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model_3.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_3(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluating the model\n",
        "model_3.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_3(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y0Z3Dl-iwgl",
        "outputId": "3a6e4b46-6a36-4346-bd43-44d8fccc43ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.4694708731770516\n",
            "Epoch 1, Batch 200, Loss: 0.5532271987199784\n",
            "Epoch 1, Batch 300, Loss: 0.4172725349664688\n",
            "Epoch 1, Batch 400, Loss: 0.3224704061448574\n",
            "Epoch 1, Batch 500, Loss: 0.2993147809803486\n",
            "Epoch 1, Batch 600, Loss: 0.29385972194373605\n",
            "Epoch 1, Batch 700, Loss: 0.2648950580507517\n",
            "Epoch 1, Batch 800, Loss: 0.24012051679193974\n",
            "Epoch 1, Batch 900, Loss: 0.2307078104838729\n",
            "Epoch 2, Batch 100, Loss: 0.19984303176403045\n",
            "Epoch 2, Batch 200, Loss: 0.17190765183418988\n",
            "Epoch 2, Batch 300, Loss: 0.16417120456695555\n",
            "Epoch 2, Batch 400, Loss: 0.18073924709111452\n",
            "Epoch 2, Batch 500, Loss: 0.18270463116466998\n",
            "Epoch 2, Batch 600, Loss: 0.17430584598332644\n",
            "Epoch 2, Batch 700, Loss: 0.16245834382250904\n",
            "Epoch 2, Batch 800, Loss: 0.15987214295193553\n",
            "Epoch 2, Batch 900, Loss: 0.1543268659710884\n",
            "Epoch 3, Batch 100, Loss: 0.12917786138132215\n",
            "Epoch 3, Batch 200, Loss: 0.14223155837506055\n",
            "Epoch 3, Batch 300, Loss: 0.12344783972948789\n",
            "Epoch 3, Batch 400, Loss: 0.1154082040861249\n",
            "Epoch 3, Batch 500, Loss: 0.14686898211017252\n",
            "Epoch 3, Batch 600, Loss: 0.11957929775118828\n",
            "Epoch 3, Batch 700, Loss: 0.11686017088592053\n",
            "Epoch 3, Batch 800, Loss: 0.11789985878393054\n",
            "Epoch 3, Batch 900, Loss: 0.14074592975899577\n",
            "Epoch 4, Batch 100, Loss: 0.1045520933996886\n",
            "Epoch 4, Batch 200, Loss: 0.10415400516241789\n",
            "Epoch 4, Batch 300, Loss: 0.09475117767229677\n",
            "Epoch 4, Batch 400, Loss: 0.11358510392252356\n",
            "Epoch 4, Batch 500, Loss: 0.10760252870619297\n",
            "Epoch 4, Batch 600, Loss: 0.09167065703310072\n",
            "Epoch 4, Batch 700, Loss: 0.11250768925994635\n",
            "Epoch 4, Batch 800, Loss: 0.09362494077533484\n",
            "Epoch 4, Batch 900, Loss: 0.09671922801993788\n",
            "Epoch 5, Batch 100, Loss: 0.08958480360452085\n",
            "Epoch 5, Batch 200, Loss: 0.09561667044647038\n",
            "Epoch 5, Batch 300, Loss: 0.07947105379309505\n",
            "Epoch 5, Batch 400, Loss: 0.09322207992896438\n",
            "Epoch 5, Batch 500, Loss: 0.09815423754043877\n",
            "Epoch 5, Batch 600, Loss: 0.09063314213883132\n",
            "Epoch 5, Batch 700, Loss: 0.08810834291391074\n",
            "Epoch 5, Batch 800, Loss: 0.10003964836709202\n",
            "Epoch 5, Batch 900, Loss: 0.07300961988978089\n",
            "Finished Training\n",
            "Accuracy on test set: 0.9627%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Result:</b>\n",
        "Model showed a good performance and by changing the activation function the accuracy neither improved nor deprived. So we can say that this is also a good combination to use.\n"
      ],
      "metadata": {
        "id": "VHrcjAdrkgcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Hypothesis 3:</b>\n",
        "\n",
        "Using a Dropout Layer with adjustable dropout rate just to check how it effects the performance of the model."
      ],
      "metadata": {
        "id": "T_3uHYnrk5al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_4(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(MLP_4, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with adjustable dropout rate\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Applying dropout\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initializing model_4 with dropout rate of 0.5\n",
        "model_4 = MLP_4(dropout_rate=0.5)"
      ],
      "metadata": {
        "id": "NWfxN3HBixOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=0.001)\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model_4.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_4(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluating the model\n",
        "model_4.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_4(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZYT2ISym0ML",
        "outputId": "a55a3ff0-9be7-4d6d-ff4f-79d993c8c3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.4247095739841462\n",
            "Epoch 1, Batch 200, Loss: 0.6364197665452958\n",
            "Epoch 1, Batch 300, Loss: 0.4884757977724075\n",
            "Epoch 1, Batch 400, Loss: 0.43475090578198433\n",
            "Epoch 1, Batch 500, Loss: 0.4098598180711269\n",
            "Epoch 1, Batch 600, Loss: 0.3718586927652359\n",
            "Epoch 1, Batch 700, Loss: 0.32807513639330865\n",
            "Epoch 1, Batch 800, Loss: 0.32316673986613753\n",
            "Epoch 1, Batch 900, Loss: 0.30640654511749743\n",
            "Epoch 2, Batch 100, Loss: 0.2576712003350258\n",
            "Epoch 2, Batch 200, Loss: 0.2510404726117849\n",
            "Epoch 2, Batch 300, Loss: 0.2546730499714613\n",
            "Epoch 2, Batch 400, Loss: 0.24554114561527968\n",
            "Epoch 2, Batch 500, Loss: 0.23934019446372987\n",
            "Epoch 2, Batch 600, Loss: 0.2207339145988226\n",
            "Epoch 2, Batch 700, Loss: 0.21583999544382096\n",
            "Epoch 2, Batch 800, Loss: 0.21956197522580623\n",
            "Epoch 2, Batch 900, Loss: 0.22323226053267717\n",
            "Epoch 3, Batch 100, Loss: 0.18803455881774425\n",
            "Epoch 3, Batch 200, Loss: 0.1969586317613721\n",
            "Epoch 3, Batch 300, Loss: 0.19588945291936397\n",
            "Epoch 3, Batch 400, Loss: 0.1861816007271409\n",
            "Epoch 3, Batch 500, Loss: 0.18242897398769856\n",
            "Epoch 3, Batch 600, Loss: 0.1733229461312294\n",
            "Epoch 3, Batch 700, Loss: 0.1913978947326541\n",
            "Epoch 3, Batch 800, Loss: 0.16907049966976045\n",
            "Epoch 3, Batch 900, Loss: 0.1793980981595814\n",
            "Epoch 4, Batch 100, Loss: 0.16489396775141357\n",
            "Epoch 4, Batch 200, Loss: 0.1569340771995485\n",
            "Epoch 4, Batch 300, Loss: 0.16189097408205272\n",
            "Epoch 4, Batch 400, Loss: 0.16714908177033067\n",
            "Epoch 4, Batch 500, Loss: 0.15100565750151873\n",
            "Epoch 4, Batch 600, Loss: 0.1564502436481416\n",
            "Epoch 4, Batch 700, Loss: 0.15670031399466097\n",
            "Epoch 4, Batch 800, Loss: 0.16483446110039948\n",
            "Epoch 4, Batch 900, Loss: 0.16732915984466673\n",
            "Epoch 5, Batch 100, Loss: 0.14129401719197632\n",
            "Epoch 5, Batch 200, Loss: 0.12732081764377653\n",
            "Epoch 5, Batch 300, Loss: 0.13756397662684322\n",
            "Epoch 5, Batch 400, Loss: 0.13189945551566779\n",
            "Epoch 5, Batch 500, Loss: 0.1323945033363998\n",
            "Epoch 5, Batch 600, Loss: 0.1476115632802248\n",
            "Epoch 5, Batch 700, Loss: 0.1415780868800357\n",
            "Epoch 5, Batch 800, Loss: 0.14750694930553437\n",
            "Epoch 5, Batch 900, Loss: 0.1393739937338978\n",
            "Finished Training\n",
            "Accuracy on test set: 0.9645%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Result</b>\n",
        "The accuracy did not change when compared with the previous models. It is in the same range, so by adding a dropout layer and using the same combination of layers does not impact the performance of the model."
      ],
      "metadata": {
        "id": "i5qutIuCm_e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Hypothesis 4</b>\n",
        "\n",
        "Changing the number of epochs"
      ],
      "metadata": {
        "id": "mf8OEU97m_j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_5, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initializing model_5\n",
        "model_5 = MLP_5()\n",
        "\n"
      ],
      "metadata": {
        "id": "HE0fCS4Sm-fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_5.parameters(), lr=0.001)\n",
        "\n",
        "# Training the neural network\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    model_5.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_5(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluating the model\n",
        "model_5.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_5(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xnr_5zHm-in",
        "outputId": "5f600211-8eb6-4def-dd13-478917c97570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.1310789316892624\n",
            "Epoch 1, Batch 200, Loss: 0.48800792306661606\n",
            "Epoch 1, Batch 300, Loss: 0.40975818306207656\n",
            "Epoch 1, Batch 400, Loss: 0.32280451729893683\n",
            "Epoch 1, Batch 500, Loss: 0.31080727465450764\n",
            "Epoch 1, Batch 600, Loss: 0.302058434933424\n",
            "Epoch 1, Batch 700, Loss: 0.25768276520073413\n",
            "Epoch 1, Batch 800, Loss: 0.2520501197502017\n",
            "Epoch 1, Batch 900, Loss: 0.2356016906350851\n",
            "Epoch 2, Batch 100, Loss: 0.22867143228650094\n",
            "Epoch 2, Batch 200, Loss: 0.19990795232355596\n",
            "Epoch 2, Batch 300, Loss: 0.19736208997666835\n",
            "Epoch 2, Batch 400, Loss: 0.18952029017731548\n",
            "Epoch 2, Batch 500, Loss: 0.16513678643852472\n",
            "Epoch 2, Batch 600, Loss: 0.17391017835587264\n",
            "Epoch 2, Batch 700, Loss: 0.1533302374370396\n",
            "Epoch 2, Batch 800, Loss: 0.17202186483889817\n",
            "Epoch 2, Batch 900, Loss: 0.15947617204859854\n",
            "Epoch 3, Batch 100, Loss: 0.12996052405796946\n",
            "Epoch 3, Batch 200, Loss: 0.13731842467561364\n",
            "Epoch 3, Batch 300, Loss: 0.14077948115766048\n",
            "Epoch 3, Batch 400, Loss: 0.1354887911491096\n",
            "Epoch 3, Batch 500, Loss: 0.13020684758201242\n",
            "Epoch 3, Batch 600, Loss: 0.13773678086698055\n",
            "Epoch 3, Batch 700, Loss: 0.12900285065174102\n",
            "Epoch 3, Batch 800, Loss: 0.12912037070840598\n",
            "Epoch 3, Batch 900, Loss: 0.12051516488194465\n",
            "Epoch 4, Batch 100, Loss: 0.10191998040303588\n",
            "Epoch 4, Batch 200, Loss: 0.10180232216604054\n",
            "Epoch 4, Batch 300, Loss: 0.11196663913317025\n",
            "Epoch 4, Batch 400, Loss: 0.11452782343141735\n",
            "Epoch 4, Batch 500, Loss: 0.11378850830718874\n",
            "Epoch 4, Batch 600, Loss: 0.11307280277833343\n",
            "Epoch 4, Batch 700, Loss: 0.0922730816155672\n",
            "Epoch 4, Batch 800, Loss: 0.11587172288447618\n",
            "Epoch 4, Batch 900, Loss: 0.10876272981055081\n",
            "Epoch 5, Batch 100, Loss: 0.08200929803308099\n",
            "Epoch 5, Batch 200, Loss: 0.09496734167449176\n",
            "Epoch 5, Batch 300, Loss: 0.08950758304446936\n",
            "Epoch 5, Batch 400, Loss: 0.1042082801926881\n",
            "Epoch 5, Batch 500, Loss: 0.09447318885475398\n",
            "Epoch 5, Batch 600, Loss: 0.08993534322828055\n",
            "Epoch 5, Batch 700, Loss: 0.09201197551097721\n",
            "Epoch 5, Batch 800, Loss: 0.07952300575561821\n",
            "Epoch 5, Batch 900, Loss: 0.09312533935531973\n",
            "Epoch 6, Batch 100, Loss: 0.0771769839990884\n",
            "Epoch 6, Batch 200, Loss: 0.0792513048928231\n",
            "Epoch 6, Batch 300, Loss: 0.08069278276525438\n",
            "Epoch 6, Batch 400, Loss: 0.07260394285898655\n",
            "Epoch 6, Batch 500, Loss: 0.07230399913154542\n",
            "Epoch 6, Batch 600, Loss: 0.08448024074546993\n",
            "Epoch 6, Batch 700, Loss: 0.0809763624239713\n",
            "Epoch 6, Batch 800, Loss: 0.08546611401252449\n",
            "Epoch 6, Batch 900, Loss: 0.08174741370603442\n",
            "Epoch 7, Batch 100, Loss: 0.05662491826573387\n",
            "Epoch 7, Batch 200, Loss: 0.07836169623304158\n",
            "Epoch 7, Batch 300, Loss: 0.06958997898735106\n",
            "Epoch 7, Batch 400, Loss: 0.06666893667541444\n",
            "Epoch 7, Batch 500, Loss: 0.06940670949406921\n",
            "Epoch 7, Batch 600, Loss: 0.07695160008501262\n",
            "Epoch 7, Batch 700, Loss: 0.07432228129822761\n",
            "Epoch 7, Batch 800, Loss: 0.07505359593546018\n",
            "Epoch 7, Batch 900, Loss: 0.06747810000553728\n",
            "Epoch 8, Batch 100, Loss: 0.06632772432640195\n",
            "Epoch 8, Batch 200, Loss: 0.050224244880955664\n",
            "Epoch 8, Batch 300, Loss: 0.07818430379731581\n",
            "Epoch 8, Batch 400, Loss: 0.057770546772517264\n",
            "Epoch 8, Batch 500, Loss: 0.056223498377949\n",
            "Epoch 8, Batch 600, Loss: 0.0770222811982967\n",
            "Epoch 8, Batch 700, Loss: 0.0603274578275159\n",
            "Epoch 8, Batch 800, Loss: 0.06560333637520671\n",
            "Epoch 8, Batch 900, Loss: 0.0677791267959401\n",
            "Epoch 9, Batch 100, Loss: 0.059748241920024155\n",
            "Epoch 9, Batch 200, Loss: 0.04537277540424839\n",
            "Epoch 9, Batch 300, Loss: 0.052777188676409424\n",
            "Epoch 9, Batch 400, Loss: 0.0455487566976808\n",
            "Epoch 9, Batch 500, Loss: 0.0588114345917711\n",
            "Epoch 9, Batch 600, Loss: 0.06119454302126542\n",
            "Epoch 9, Batch 700, Loss: 0.07261742373462766\n",
            "Epoch 9, Batch 800, Loss: 0.06402405653148889\n",
            "Epoch 9, Batch 900, Loss: 0.0713070643437095\n",
            "Epoch 10, Batch 100, Loss: 0.05835820107255131\n",
            "Epoch 10, Batch 200, Loss: 0.04078937267884612\n",
            "Epoch 10, Batch 300, Loss: 0.04613668701378629\n",
            "Epoch 10, Batch 400, Loss: 0.05798963330569677\n",
            "Epoch 10, Batch 500, Loss: 0.0527814419940114\n",
            "Epoch 10, Batch 600, Loss: 0.05315796297043562\n",
            "Epoch 10, Batch 700, Loss: 0.06125188825652003\n",
            "Epoch 10, Batch 800, Loss: 0.050580489269923416\n",
            "Epoch 10, Batch 900, Loss: 0.0464354812214151\n",
            "Epoch 11, Batch 100, Loss: 0.039983610396739096\n",
            "Epoch 11, Batch 200, Loss: 0.04270729313138873\n",
            "Epoch 11, Batch 300, Loss: 0.05204221975640394\n",
            "Epoch 11, Batch 400, Loss: 0.05002358023892157\n",
            "Epoch 11, Batch 500, Loss: 0.05782858737744391\n",
            "Epoch 11, Batch 600, Loss: 0.05103987666661851\n",
            "Epoch 11, Batch 700, Loss: 0.0470539337315131\n",
            "Epoch 11, Batch 800, Loss: 0.05042223808937706\n",
            "Epoch 11, Batch 900, Loss: 0.05781257049180567\n",
            "Epoch 12, Batch 100, Loss: 0.04177288950420916\n",
            "Epoch 12, Batch 200, Loss: 0.028798052003839985\n",
            "Epoch 12, Batch 300, Loss: 0.05172012328810524\n",
            "Epoch 12, Batch 400, Loss: 0.039968428660649806\n",
            "Epoch 12, Batch 500, Loss: 0.040409048971487206\n",
            "Epoch 12, Batch 600, Loss: 0.04076636050827801\n",
            "Epoch 12, Batch 700, Loss: 0.04718863342655823\n",
            "Epoch 12, Batch 800, Loss: 0.04751912952400744\n",
            "Epoch 12, Batch 900, Loss: 0.04398905731854029\n",
            "Epoch 13, Batch 100, Loss: 0.03954131131293252\n",
            "Epoch 13, Batch 200, Loss: 0.04036913839401677\n",
            "Epoch 13, Batch 300, Loss: 0.04676777000539005\n",
            "Epoch 13, Batch 400, Loss: 0.04409465419361368\n",
            "Epoch 13, Batch 500, Loss: 0.04889218848198652\n",
            "Epoch 13, Batch 600, Loss: 0.028286275394493715\n",
            "Epoch 13, Batch 700, Loss: 0.048910322181181985\n",
            "Epoch 13, Batch 800, Loss: 0.04140115760848857\n",
            "Epoch 13, Batch 900, Loss: 0.03546474897360895\n",
            "Epoch 14, Batch 100, Loss: 0.03608620770741254\n",
            "Epoch 14, Batch 200, Loss: 0.03763446138356812\n",
            "Epoch 14, Batch 300, Loss: 0.034209293923922815\n",
            "Epoch 14, Batch 400, Loss: 0.03927409953903407\n",
            "Epoch 14, Batch 500, Loss: 0.04581608637585305\n",
            "Epoch 14, Batch 600, Loss: 0.04105007947946433\n",
            "Epoch 14, Batch 700, Loss: 0.03043221151237958\n",
            "Epoch 14, Batch 800, Loss: 0.045982687673531475\n",
            "Epoch 14, Batch 900, Loss: 0.04003022321965546\n",
            "Epoch 15, Batch 100, Loss: 0.02940095428872155\n",
            "Epoch 15, Batch 200, Loss: 0.03371166425087722\n",
            "Epoch 15, Batch 300, Loss: 0.043933162752073256\n",
            "Epoch 15, Batch 400, Loss: 0.04172059093485586\n",
            "Epoch 15, Batch 500, Loss: 0.03241709907204495\n",
            "Epoch 15, Batch 600, Loss: 0.034126583972829395\n",
            "Epoch 15, Batch 700, Loss: 0.029811515605542807\n",
            "Epoch 15, Batch 800, Loss: 0.03740137903776485\n",
            "Epoch 15, Batch 900, Loss: 0.04456670477578882\n",
            "Finished Training\n",
            "Accuracy on test set: 0.9677%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "By Increasing the number of epochs the accuracy increased a bit from the previous model, but it falls under the same range, so increasing of epochs also does not have much impact on the model.\n",
        "\n",
        "The combination which we used in the model_2 is the best one. The model's performance is improved by adding a dense layer.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A8OGwz4lN5Tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GitHub Link\n",
        "\n",
        "https://github.com/sweekruthi-balivada/Deep-Learning\n"
      ],
      "metadata": {
        "id": "l85Dv7p_3Id7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GO1Ae_lq3Nma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}