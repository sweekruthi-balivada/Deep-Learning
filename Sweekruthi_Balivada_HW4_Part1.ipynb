{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKYP1YoZAIBT",
        "outputId": "ab031f82-a032-4095-d741-4903c550cd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.3/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.20\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ],
      "metadata": {
        "id": "2DU3HBbnAT23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_zUhdLI-AZ_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcYqB83YAfKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "DiRHU0FmAfQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks\n",
        "\n",
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNaVOzqNDDO3",
        "outputId": "63eb27d8-884f-4299-f16e-770d45640e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.7/107.7 GB disk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 149MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On all the frames"
      ],
      "metadata": {
        "id": "JncTOvmJYcfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On Training Images"
      ],
      "metadata": {
        "id": "m8ILaOoEYiPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Set up directories\n",
        "input_folder = '/content/drive/MyDrive/train_images'  # Folder containing frames\n",
        "output_folder = '/content/drive/MyDrive/annotated_frames'  # Folder to store annotated frames\n",
        "output_video_path = '/content/drive/MyDrive/train_annotated_video.mp4'  # Output video path\n",
        "\n",
        "# Process each frame\n",
        "for frame_file in tqdm(sorted(Path(input_folder).glob('*.jpg'))):  # Only process JPEG images\n",
        "    frame_name = frame_file.stem\n",
        "    frame_path = str(frame_file)\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame_path)\n",
        "\n",
        "    # Draw bounding boxes around detected vehicles\n",
        "    annotated_frame = results.render()[0]\n",
        "\n",
        "    # Save the annotated frame\n",
        "    output_path = Path(output_folder) / f'{frame_name}.jpg'\n",
        "    cv2.imwrite(str(output_path), annotated_frame)\n",
        "\n",
        "# Compile annotated frames into a video\n",
        "frames = [cv2.imread(str(frame)) for frame in sorted(Path(output_folder).glob('*'))]\n",
        "frame_height, frame_width, _ = frames[0].shape\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(\"Video created successfully:\", output_video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3xNNK_3Llcm",
        "outputId": "657ae28a-9fb1-4afc-ed62-894481d965bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:12<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video created successfully: /content/drive/MyDrive/train_annotated_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model for using it on Testing data"
      ],
      "metadata": {
        "id": "MceDZFcEYl82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' is your trained YOLOv5 model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/train_images/train.pt')\n"
      ],
      "metadata": {
        "id": "KOEaQhlvMQAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## on Testing data"
      ],
      "metadata": {
        "id": "Z9GBsiKAYr9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Set up directories\n",
        "input_folder = '/content/drive/MyDrive/test_images'  # Folder containing frames\n",
        "output_folder = '/content/drive/MyDrive/test_annotated_frames'  # Folder to store annotated frames\n",
        "output_video_path = '/content/drive/MyDrive/test_annotated_video.mp4'  # Output video path\n",
        "\n",
        "# Process each frame\n",
        "for frame_file in tqdm(sorted(Path(input_folder).glob('*.jpg'))):  # Only process JPEG images\n",
        "    frame_name = frame_file.stem\n",
        "    frame_path = str(frame_file)\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame_path)\n",
        "\n",
        "    # Draw bounding boxes around detected vehicles\n",
        "    annotated_frame = results.render()[0]\n",
        "\n",
        "    # Save the annotated frame\n",
        "    output_path = Path(output_folder) / f'{frame_name}.jpg'\n",
        "    cv2.imwrite(str(output_path), annotated_frame)\n",
        "\n",
        "# Compile annotated frames into a video\n",
        "frames = [cv2.imread(str(frame)) for frame in sorted(Path(output_folder).glob('*'))]\n",
        "frame_height, frame_width, _ = frames[0].shape\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(\"Video created successfully:\", output_video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_egpLm0RLlgN",
        "outputId": "0c4b1a93-6cbd-4d9c-a4b8-bcdcb972e2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:06<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video created successfully: /content/drive/MyDrive/test_annotated_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kz6xdgqWOq5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0yHMin0XNZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}